{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNDoIl8OUAdcnx2QYVAM+c5"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9397341,"sourceType":"datasetVersion","datasetId":5703698},{"sourceId":9397390,"sourceType":"datasetVersion","datasetId":5703727},{"sourceId":9398066,"sourceType":"datasetVersion","datasetId":5704232},{"sourceId":9401441,"sourceType":"datasetVersion","datasetId":5707024},{"sourceId":9401464,"sourceType":"datasetVersion","datasetId":5707044},{"sourceId":9401625,"sourceType":"datasetVersion","datasetId":5707178},{"sourceId":113736,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":95452,"modelId":119647},{"sourceId":114020,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":95703,"modelId":119891}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"EAST MODEL Implementation","metadata":{"id":"b67DhyCewyXa"}},{"cell_type":"code","source":"!pip install opencv-python pytesseract\n","metadata":{"id":"7XHEmCM9W0jZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726356769317,"user_tz":-330,"elapsed":3923,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"outputId":"2dcc95cf-c48f-4823-9fd5-cef49035b05f","execution":{"iopub.status.busy":"2024-09-16T05:39:37.582714Z","iopub.execute_input":"2024-09-16T05:39:37.583172Z","iopub.status.idle":"2024-09-16T05:39:52.205221Z","shell.execute_reply.started":"2024-09-16T05:39:37.583131Z","shell.execute_reply":"2024-09-16T05:39:52.203987Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\nRequirement already satisfied: pytesseract in /opt/conda/lib/python3.10/site-packages (0.3.13)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (21.3)\nRequirement already satisfied: Pillow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.3->pytesseract) (3.1.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install imutils\n\nimport numpy as np\nimport cv2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fj7RxFPHTJVd","executionInfo":{"status":"ok","timestamp":1726356780193,"user_tz":-330,"elapsed":5192,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"outputId":"bd046a0e-2f9e-4b43-bf71-92560ddf8c3f","execution":{"iopub.status.busy":"2024-09-16T05:39:52.207545Z","iopub.execute_input":"2024-09-16T05:39:52.207980Z","iopub.status.idle":"2024-09-16T05:40:08.458344Z","shell.execute_reply.started":"2024-09-16T05:39:52.207930Z","shell.execute_reply":"2024-09-16T05:40:08.457497Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25837 sha256=8bf526087d1f5fa5f803c827586fce14a8aa7a37a105517950467092b6a92b9f\n  Stored in directory: /root/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"IMAGE PROCESSING","metadata":{"id":"x9wSQczsw4CF"}},{"cell_type":"code","source":"from PIL import Image","metadata":{"id":"sC2haUYXFDkc","executionInfo":{"status":"ok","timestamp":1726356780194,"user_tz":-330,"elapsed":20,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"execution":{"iopub.status.busy":"2024-09-16T05:40:08.459619Z","iopub.execute_input":"2024-09-16T05:40:08.459967Z","iopub.status.idle":"2024-09-16T05:40:08.464880Z","shell.execute_reply.started":"2024-09-16T05:40:08.459928Z","shell.execute_reply":"2024-09-16T05:40:08.463923Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def denoise_image(img):\n    # Apply a Gaussian Blur with a kernel size of (5, 5)\n    denoised = cv2.GaussianBlur(img, (5, 5), 0)\n    return denoised\n\ndef deblur_image(img,strength):\n    kernel = np.array([[0, -strength, 0],\n                       [-strength, 1+4*strength, -strength],\n                       [0, -strength, 0]])\n    deblurred = cv2.filter2D(img, -1, kernel)\n    return deblurred\n# Step 3: Enlarge using PIL’s LANCZOS interpolation\ndef enlarge_image(img, new_width, new_height):\n    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n\n    enlarged = img_pil.resize((new_width, new_height), Image.LANCZOS)\n\n    enlarged_cv2 = cv2.cvtColor(np.array(enlarged), cv2.COLOR_RGB2BGR)\n    return enlarged_cv2\n\n# Step 4: Sharpen the enlarged image (less intense sharpening)\ndef sharpen_image(img,strength):\n    kernel = np.array([[0, -strength, 0],\n                       [-strength, 1+4*strength, -strength],\n                       [0, -strength, 0]])\n    sharpened = cv2.filter2D(img, -1, kernel)\n    return sharpened\n\n# Step 5: Adjust contrast using CLAHE (less aggressive)\ndef adjust_contrast(img):\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    clahe = cv2.createCLAHE(clipLimit=0.5, tileGridSize=(8, 8))  # Lower contrast adjustment\n    lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n    contrast_enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n    return contrast_enhanced\n\ndef add_padding(img, padding_size):\n    padded_image = cv2.copyMakeBorder(\n        img, padding_size, padding_size, padding_size, padding_size,\n        cv2.BORDER_REPLICATE\n    )\n    h, w, c = padded_image.shape\n\n    for i in range(padding_size):\n        padded_image[:, i] =padded_image[:, padding_size+2]\n\n    return padded_image\n\n\n# Full pipeline\ndef enhance_image(image, new_width, new_height,s):\n    # Step 1: Denoise\n    denoised_img = denoise_image(image)\n\n    # Step 2: Deblur (Sharpening as basic deblurring)\n    deblurred_img = deblur_image(denoised_img,s)\n\n    # Step 3: Enlarge using PIL’s LANCZOS\n    enlarged_img = enlarge_image(deblurred_img, new_width, new_height)\n\n    # Step 4: Sharpen the enlarged image\n    sharpened_img = sharpen_image(enlarged_img,s)\n\n    # Step 5: Adjust contrast\n    final_image = adjust_contrast(sharpened_img)\n\n    f1 = add_padding(final_image, 3)\n    f2 = add_padding(f1, 3)\n    f3 = add_padding(f2, 4)\n\n    return f3","metadata":{"execution":{"iopub.status.busy":"2024-09-16T05:40:08.466969Z","iopub.execute_input":"2024-09-16T05:40:08.467302Z","iopub.status.idle":"2024-09-16T05:40:08.482199Z","shell.execute_reply.started":"2024-09-16T05:40:08.467269Z","shell.execute_reply":"2024-09-16T05:40:08.481234Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def process_img(img):\n    h,w =(img.shape)[:2]\n    w_scale=1000/w\n    h_scale=1000/h\n    mini=min(w_scale,h_scale)\n    new_height = int(h)  # Define the new height for enlargement\n    new_width = int(w)  # Define the new width for enlargement\n    if(mini>1):\n      new_width=int(h*mini)\n      new_height=int(w*mini)\n\n    output=enhance_image(img, new_width, new_height,0.017)\n    return output","metadata":{"execution":{"iopub.status.busy":"2024-09-16T05:40:08.483486Z","iopub.execute_input":"2024-09-16T05:40:08.484100Z","iopub.status.idle":"2024-09-16T05:40:08.492844Z","shell.execute_reply.started":"2024-09-16T05:40:08.484055Z","shell.execute_reply":"2024-09-16T05:40:08.492006Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def process_YOLO_sub_image(img, xmin, xmax, ymin, ymax, scale_factor=4):\n    center_x = (xmin + xmax) // 2\n    center_y = (ymin + ymax) // 2\n\n    # Step 2: Calculate the width and height of the bounding box\n    width = xmax - xmin\n    height = ymax - ymin\n\n    # Step 3: Enlarge the width and height by the scale factor\n    new_width = int(width * scale_factor)\n    new_height = int(height * scale_factor)\n\n    # Step 4: Calculate the new xmin, xmax, ymin, ymax keeping the center constant\n    new_xmin = center_x - new_width // 2\n    new_xmax = center_x + new_width // 2\n    new_ymin = center_y - new_height // 2\n    new_ymax = center_y + new_height // 2\n\n    # Ensure the new coordinates are within the image boundaries\n    new_xmin = max(0, new_xmin)\n    new_xmax = min(img.shape[1], new_xmax)\n    new_ymin = max(0, new_ymin)\n    new_ymax = min(img.shape[0], new_ymax)\n\n    # Step 5: Extract the enlarged region from the original image\n    enlarged_region = img[new_ymin:new_ymax, new_xmin:new_xmax]\n\n    return enlarged_region","metadata":{"execution":{"iopub.status.busy":"2024-09-16T05:40:08.493959Z","iopub.execute_input":"2024-09-16T05:40:08.494290Z","iopub.status.idle":"2024-09-16T05:40:08.502753Z","shell.execute_reply.started":"2024-09-16T05:40:08.494257Z","shell.execute_reply":"2024-09-16T05:40:08.501922Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"EasyOCR Implementation","metadata":{"id":"J0qUuaXIw-5I"}},{"cell_type":"code","source":"!pip install easyocr\n!pip install torch torchvision torchaudio\n\nimport torch\n!pip install timm\nimport timm  # For loading the Swin Transformer\nimport cv2\nimport os\nimport torch.nn as nn\n\n!pip install opencv-python-headless matplotlib\n\nimport easyocr\n\nimport matplotlib.pyplot as plt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sx1zKz37wI8q","executionInfo":{"status":"ok","timestamp":1726356806952,"user_tz":-330,"elapsed":24657,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"outputId":"383bb583-9e1b-43f7-d4ea-59fd245eb3aa","execution":{"iopub.status.busy":"2024-09-16T05:40:08.504050Z","iopub.execute_input":"2024-09-16T05:40:08.504653Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: easyocr in /opt/conda/lib/python3.10/site-packages (1.7.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from easyocr) (2.4.0)\nRequirement already satisfied: torchvision>=0.5 in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.19.0)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from easyocr) (4.10.0.84)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from easyocr) (9.5.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.23.2)\nRequirement already satisfied: python-bidi in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.6.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from easyocr) (6.0.2)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.8.5.post1)\nRequirement already satisfied: pyclipper in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.3.0.post5)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.11.1.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (3.3)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (1.13.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->easyocr) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->easyocr) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->easyocr) (1.3.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (1.0.8)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.19.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.24.6)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"reader = easyocr.Reader(['en'], gpu=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_text(image_pixel):\n\n    # Detect text from the image\n    results = reader.readtext(image_pixel)\n\n    # Loop over the detected text boxes and draw them on the image\n    full = ''\n    for (bbox, text, prob) in results:\n        # Unpack the bounding box coordinates\n        if text is not None:\n            if prob >= 0.3:\n                full += text + ' '\n    return full\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize EasyOCR reader\n\n# def recognize_text_with_easyocr(image_pixels):\n#     result = reader.readtext(image_pixels)\n#     \"\"\" Recognize text using EasyOCR \"\"\"\n#     for (bbox, text, prob) in result:\n#         if text is not None:\n#           if prob < 0.2 :\n#             updated_image_pixels = image_processing(image_pixels)\n#             result_new = reader.readtext(updated_image_pixels)\n#             for (bbox, text, prob) in result_new:\n#               if text is not None:\n#                 if prob >= 0.2:\n#                   return text\n#           else:\n#             return text\n#         print(\"none detected\")\n#     return \"\"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_y_K4TlduogQ","executionInfo":{"status":"ok","timestamp":1726356812151,"user_tz":-330,"elapsed":5233,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"outputId":"ea079650-b7d5-4b67-8260-2a0704e5e13b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"YOLO Model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install ultralytics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model weights in Kaggle\nfrom ultralytics import YOLO\n\n# Load the model from the uploaded file (adjust the path as necessary)\nmodel = YOLO('/kaggle/input/yolo-model/best.pt')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image_pixels):\n    # Convert image to RGB (YOLO expects RGB, OpenCV loads images in BGR)\n    image_rgb = cv2.cvtColor(image_pixels, cv2.COLOR_BGR2RGB)\n    \n    # Resize image to YOLO model input size (640x640 by default for YOLOv8)\n    image_resized = cv2.resize(image_rgb, (640, 640))\n    \n    # Convert to numpy array and normalize the pixel values (0-1)\n    image_resized = image_resized / 255.0\n    \n    # Transpose the image to match (channels, height, width)\n    image_transposed = image_resized.transpose(2, 0, 1)\n    \n    # Add batch dimension (1, channels, height, width)\n    image_batched = np.expand_dims(image_transposed, axis=0)\n    \n    # Convert to PyTorch tensor\n    image_tensor = torch.from_numpy(image_batched).float()\n    \n    return image_tensor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_YOLO(image_pixels):\n    results = model(image_pixels)\n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def YOLO_output(image_pixels):\n    \n    class_dict = {0: 'depth', 1: 'height', 2: 'width'}\n    prediction_dict = {'depth' : [], 'height' : [], 'width' : []}\n    \n    image_pixels = preprocess_image(image_pixels)\n    \n    results = run_YOLO(image_pixels)\n    for result in results:\n        boxes = result.boxes  # Get bounding boxes for each detected object\n        for box in boxes:\n            # Extract the coordinates and other details\n            x1, y1, x2, y2 = box.xyxy[0]  # Get the bounding box coordinates\n            \n            x1_val = int(x1.cpu().item())\n            y1_val = int(y1.cpu().item())\n            x2_val = int(x2.cpu().item())\n            y2_val = int(y2.cpu().item())\n            \n            img_new = process_YOLO_sub_image(image_pixels, x1_val, x2_val, y1_val, y2_val)\n            \n            class_id = box.cls[0]  # Get the class ID\n            confidence = box.conf[0]  # Get the confidence score\n            \n            if class_id in model.names:\n                class_name = model.names[class_id]\n            else:\n                class_name = \"Unknown class\"\n            def is_image_array(array):\n                # Check if the array is a 2D (grayscale) or 3D (color) image\n                if array.ndim == 2:  # Grayscale image\n                    return True\n                elif array.ndim == 3:\n                    # Check if the last dimension corresponds to RGB/BGR channels\n                    if array.shape[2] == 3:  # RGB or BGR\n                        return True\n    \n                return False\n\n            def check_image_format(array):\n                if not isinstance(array, np.ndarray):\n                    return False, \"Not a NumPy array\"\n\n                if is_image_array(array):\n                    return True,\n                else:\n                    return False, \"The array does not match common image formats.\"\n            \n            \n            if check_image_format(img_new) and is_image_array(img_new):\n                \n                prediction_dict[class_name] = [detect_text(img_new), confidence]\n        \n    return prediction_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Question, Context, Answer extraction","metadata":{"id":"VIB0Pyrr3vWB"}},{"cell_type":"code","source":"import pandas as pd\nimport gc\n\n# Iterate through all variables in the global namespace\nfor name in dir():\n    obj = globals()[name]\n    if isinstance(obj, pd.DataFrame):\n        del globals()[name]\n\n# Optional: Run garbage collector to free memory\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt: create new table df_qna with columns 'question', 'context', 'answer' all of string type\n\n\ndf_qna = pd.DataFrame(columns=['question', 'context', 'answer', 'unit'], dtype=str)\n","metadata":{"id":"zrAs5AwZ3uKZ","executionInfo":{"status":"ok","timestamp":1726356812152,"user_tz":-330,"elapsed":12,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/train-data/train.csv')","metadata":{"id":"euWBTmu84U72","executionInfo":{"status":"ok","timestamp":1726356812152,"user_tz":-330,"elapsed":11,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt: create a new dataframe by sampling 50 images from df where df['entity_name'] == \"item_weight\"\n\nimport pandas as pd\n\n# Sample 50 images where 'entity_name' is \"item_weight\"\ndf_sampled_weight = df[df['entity_name'] == \"item_weight\"].sample(n=100, random_state = 42)\n\ndf_sampled_max_weight = df[df['entity_name'] == \"maximum_weight_recommendation\"].sample(n=150, random_state = 42)\n\ndf_sampled_wattage = df[df['entity_name'] == \"wattage\"].sample(n=20, random_state = 42)\n\ndf_sampled_voltage = df[df['entity_name'] == \"voltage\"].sample(n=20, random_state = 42)\n\n\ndf_sampled_volume = df[df['entity_name'] == \"item_volume\"].sample(n=20, random_state = 42)\n\n\n# Create a new dataframe with the sampled data\n","metadata":{"id":"0M2DEDjP4-iO","executionInfo":{"status":"ok","timestamp":1726356812152,"user_tz":-330,"elapsed":10,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sampled_voltage.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_qna_weight = pd.DataFrame(columns=['question', 'context', 'answer', 'unit'], dtype=str)\ndf_qna_max_weight = pd.DataFrame(columns=['question', 'context', 'answer', 'unit'], dtype=str)\ndf_qna_wattage = pd.DataFrame(columns=['question', 'context', 'answer', 'unit'], dtype=str)\ndf_qna_voltage = pd.DataFrame(columns=['question', 'context', 'answer', 'unit'], dtype=str)\ndf_qna_volume = pd.DataFrame(columns=['question', 'context', 'answer', 'unit'], dtype=str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt: for each row in df_sampled, populate each row of the df_qna as follows :\n# 'question' column = \"What is the maximum weight capacity of the product?\"\n# 'context' column = row['image_link']\n# 'answer' column = row['entity_value'].split()[0]\n# '\n\nfor index, row in df_sampled_weight.iterrows():\n  df_qna_weight = pd.concat([df_qna_weight, pd.DataFrame([{'question': \"What is the weight value of the product?\",\n                          'context': row['image_link'],\n                          'answer': row['entity_value'].split()[0],\n                          'unit' : row['entity_value'].split()[-1]}])], ignore_index=True)\n\nfor index, row in df_sampled_max_weight.iterrows():\n  df_qna_max_weight = pd.concat([df_qna_max_weight, pd.DataFrame([{'question': \"What is the maximum weight capacity of the product?\",\n                          'context': row['image_link'],\n                          'answer': row['entity_value'].split()[0],\n                          'unit' : row['entity_value'].split()[-1]}])], ignore_index=True)\n\nfor index, row in df_sampled_wattage.iterrows():\n  df_qna_wattage = pd.concat([df_qna_wattage, pd.DataFrame([{'question': \"What is the wattage value of the product?\",\n                          'context': row['image_link'],\n                          'answer': row['entity_value'].split()[0],\n                          'unit' : row['entity_value'].split()[-1]}])], ignore_index=True)\n\nfor index, row in df_sampled_voltage.iterrows():\n  df_qna_voltage = pd.concat([df_qna_voltage, pd.DataFrame([{'question': \"What is the voltage value of the product?\",\n                          'context': row['image_link'],\n                          'answer': row['entity_value'].split()[0], \n                          'unit' : row['entity_value'].split()[-1]}])], ignore_index=True)\nfor index, row in df_sampled_volume.iterrows():\n  df_qna_volume = pd.concat([df_qna_volume, pd.DataFrame([{'question': \"What is the volume value of the product?\",\n                          'context': row['image_link'],\n                          'answer': row['entity_value'].split()[0], \n                          'unit' : row['entity_value'].split()[-2] + ' ' + row['entity_value'].split()[-1]}])], ignore_index=True)\n","metadata":{"id":"pN6jLXpC6AnB","executionInfo":{"status":"ok","timestamp":1726356813090,"user_tz":-330,"elapsed":948,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_volume","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Processing of Volume","metadata":{}},{"cell_type":"code","source":"import re\ndef remove_first_number(text):\n    # Split the text into words\n    words = text.split()\n    \n    # Check if the first word is a number\n    if words and re.match(r'^\\d+(\\.\\d+)?$', words[0]):  # Handles integers and floats\n        # Remove the first word\n        words = words[1:]\n    \n    # Join the remaining words back into a string\n    return ' '.join(words)\n\n# Apply the function to the 'xyz' column\ndf_qna_volume['unit'] = df_qna_volume['unit'].astype(str).apply(remove_first_number)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_volume","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_number(string):\n  try:\n    float(string)\n    return True\n  except ValueError:\n    return False\n\n# Function to clean the dataframe\ndef clean_dataframe(df):\n  df = df[df['answer'].apply(is_number)]  # Keep only rows where 'answer' is a number\n  return df\n\n# Apply the cleaning function to each dataframe\ndf_qna_weight = clean_dataframe(df_qna_weight)\ndf_qna_max_weight = clean_dataframe(df_qna_max_weight)\ndf_qna_wattage = clean_dataframe(df_qna_wattage)\ndf_qna_voltage = clean_dataframe(df_qna_voltage)\ndf_qna_volume = clean_dataframe(df_qna_volume)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_unit_map = {\n  \"width\": {\n    \"centimetre\",\n    \"foot\",\n    \"millimetre\",\n    \"metre\",\n    \"inch\",\n    \"yard\"\n  },\n  \"depth\": {\n    \"centimetre\",\n    \"foot\",\n    \"millimetre\",\n    \"metre\",\n    \"inch\",\n    \"yard\"\n  },\n  \"height\": {\n    \"centimetre\",\n    \"foot\",\n    \"millimetre\",\n    \"metre\",\n    \"inch\",\n    \"yard\"\n  },\n  \"item_weight\": {\n    \"milligram\",\n    \"kilogram\",\n    \"microgram\",\n    \"gram\",\n    \"ounce\",\n    \"ton\",\n    \"pound\"\n  },\n  \"maximum_weight_recommendation\": {\n    \"milligram\",\n    \"kilogram\",\n    \"microgram\",\n    \"gram\",\n    \"ounce\",\n    \"ton\",\n    \"pound\"\n  },\n  \"voltage\": {\n    \"millivolt\",\n    \"kilovolt\",\n    \"volt\"\n  },\n  \"wattage\": {\n    \"kilowatt\",\n    \"watt\"\n  },\n  \"item_volume\": {\n    \"cubic foot\",\n    \"microlitre\",\n    \"cup\",\n    \"fluid ounce\",\n    \"centilitre\",\n    \"imperial gallon\",\n    \"pint\",\n    \"decilitre\",\n    \"litre\",\n    \"millilitre\",\n    \"quart\",\n    \"cubic inch\",\n    \"gallon\"\n  }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_volume_units = entity_unit_map[\"item_volume\"]\n\n# Create a filtered DataFrame where the 'unit' is NOT in the width units\ndf_qna_volume = df_qna_volume[df_qna_volume['unit'].isin(item_volume_units)]\n\nwattage_units = entity_unit_map[\"wattage\"]\n\n# Create a filtered DataFrame where the 'unit' is NOT in the width units\ndf_qna_wattage = df_qna_wattage[df_qna_wattage['unit'].isin(wattage_units)]\n\nitem_weight_units = entity_unit_map[\"item_weight\"]\n\n# Create a filtered DataFrame where the 'unit' is NOT in the width units\ndf_qna_weight = df_qna_weight[df_qna_weight['unit'].isin(item_weight_units)]\n\nmax_weight_units = entity_unit_map[\"maximum_weight_recommendation\"]\n\n# Create a filtered DataFrame where the 'unit' is NOT in the width units\ndf_qna_max_weight = df_qna_max_weight[df_qna_max_weight['unit'].isin(max_weight_units)]\n\nvoltage_units = entity_unit_map[\"voltage\"]\n\n# Create a filtered DataFrame where the 'unit' is NOT in the width units\ndf_qna_voltage = df_qna_voltage[df_qna_voltage['unit'].isin(voltage_units)]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_weight","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_volume.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_weight = df_qna_weight.drop_duplicates(subset=['context'], keep='first')\ndf_qna_max_weight = df_qna_max_weight.drop_duplicates(subset=['context'], keep='first')\ndf_qna_wattage = df_qna_wattage.drop_duplicates(subset=['context'], keep='first')\ndf_qna_voltage = df_qna_voltage.drop_duplicates(subset=['context'], keep='first')\ndf_qna_volume = df_qna_volume.drop_duplicates(subset=['context'], keep='first')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_max_weight.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nimport numpy as np\nfrom io import BytesIO\n\ndef download_image_to_numpy(image_url):\n    # Download the image from the given URL\n    response = requests.get(image_url)\n\n    # Check if the download was successful\n    if response.status_code == 200:\n        # Open the image from the downloaded content\n        img = Image.open(BytesIO(response.content))\n\n        # Convert the image to a numpy array (pixel array)\n        img_array = np.array(img)\n\n        return img_array\n    else:\n        raise Exception(f\"Failed to download image, status code: {response.status_code}\")\n","metadata":{"id":"FR55pPYE9PAT","executionInfo":{"status":"ok","timestamp":1726356813091,"user_tz":-330,"elapsed":39,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Convert the NumPy array to a Pillow Imag\n\ndef get_context(image_url):\n  np = download_image_to_numpy(image_url)\n  processed_img = process_img(np)\n  return detect_text(processed_img)","metadata":{"id":"91abtGZG9phq","executionInfo":{"status":"ok","timestamp":1726356813091,"user_tz":-330,"elapsed":38,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # prompt: take the first row's context column from df_qna\n\n# first_row_context = df_qna.iloc[0]['context']\n\n# img = download_image_to_numpy(first_row_context)\n\n# processed_images = crop_image(img)\n\n# print(processed_images)\n","metadata":{"id":"PlHWj_IZCYIZ","executionInfo":{"status":"ok","timestamp":1726356813091,"user_tz":-330,"elapsed":37,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt: appapily get_context function on df['context']\n\ndf_qna_weight['context_new'] = df_qna_weight['context'].apply(get_context)\ndf_qna_max_weight['context_new'] = df_qna_max_weight['context'].apply(get_context)\ndf_qna_voltage['context_new'] = df_qna_voltage['context'].apply(get_context)\ndf_qna_wattage['context_new'] = df_qna_wattage['context'].apply(get_context)\ndf_qna_volume['context_new'] = df_qna_volume['context'].apply(get_context)\n","metadata":{"id":"or4mCVnk_s7h","executionInfo":{"status":"ok","timestamp":1726356813091,"user_tz":-330,"elapsed":36,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_weight.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_max_weight.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_volume","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna_wattage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt: in the 'answer' column, if the string is of the form somenumber.000.. then replace it with somenumber\ndef remove_trailing_zeros(value):\n    # Check if the value is a string\n    if not isinstance(value, str):\n        raise ValueError(\"Input must be a string\")\n\n    # Split the string by the decimal point\n    parts = value.split('.')\n\n    # If there is no decimal point or there are no trailing zeros, return the original string\n    if len(parts) == 1:\n        return value\n    else:\n        # Remove trailing zeros from the fractional part\n        integer_part, fractional_part = parts\n        fractional_part = fractional_part.rstrip('0')\n\n        # If the fractional part is empty after stripping zeros, return just the integer part\n        if not fractional_part:\n            return integer_part\n        else:\n            # Otherwise, return the combined integer and non-zero fractional part\n            return f\"{integer_part}.{fractional_part}\"\n\ndf_qna_weight['answer'] = df_qna_weight['answer'].apply(remove_trailing_zeros)\ndf_qna_max_weight['answer'] = df_qna_max_weight['answer'].apply(remove_trailing_zeros)\ndf_qna_wattage['answer'] = df_qna_wattage['answer'].apply(remove_trailing_zeros)\ndf_qna_voltage['answer'] = df_qna_voltage['answer'].apply(remove_trailing_zeros)\ndf_qna_volume['answer'] = df_qna_volume['answer'].apply(remove_trailing_zeros)\n","metadata":{"id":"edhb4vJElby3","executionInfo":{"status":"ok","timestamp":1726356813091,"user_tz":-330,"elapsed":35,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt: create new df_qna with concatenation of all rows of df_qna_weight, df_qna_max_weight, df_qna_wattage, df_qna_voltage (which increases rows of the df not columns).\n\ndf_qna = pd.concat([df_qna_weight, df_qna_max_weight, df_qna_wattage, df_qna_voltage, df_qna_volume], ignore_index=True)\n","metadata":{"id":"4dAVGzIzu0_W","executionInfo":{"status":"ok","timestamp":1726356813091,"user_tz":-330,"elapsed":34,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna.describe()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"VrNX-QoyvMBW","executionInfo":{"status":"ok","timestamp":1726356813091,"user_tz":-330,"elapsed":34,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"outputId":"15ec9c4d-2507-45bd-c3ea-ebc4f1b73f79","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna['context_new'] = df_qna['context_new'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_qna.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Question Answer Model Fine Tuning","metadata":{"id":"EADDyIjm3w7j"}},{"cell_type":"code","source":"# prompt: For each row in df_qna add to a list train_data the following dictionary:\n# {'context':  row[''context_new'],\n#  'qas': [{'id': '00001',\n#    'is_impossible': True,\n#    'question': row['question'],\n#    'answers': [{'text': row['answer']', 'answer_start': start index of row['answer'] in row[''context_new']}]},\n\ntrain_data = []\nfor link in df_qna['context'].unique():\n    qas = []\n    filtered_df = df_qna[df_qna['context'] == link]\n    for index, row in filtered_df.iterrows():\n      context = row['context_new']\n      answer_start = context.find(str(row['answer']))\n      if answer_start == -1:\n        continue\n      qas.append({\n          'id': str(len(qas) + 1).zfill(5),\n          'is_impossible': False,\n          'question': row['question'],\n          'answers': [{'text': row['answer'], 'answer_start': answer_start}]\n      })\n    train_data.append({\n      'context': context,\n      'qas': qas})\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":537},"id":"-R723LOCgFpG","executionInfo":{"status":"error","timestamp":1726356813092,"user_tz":-330,"elapsed":30,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"outputId":"c7b5b53b-d408-45ca-b9d5-5875ab6390d7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_qna['context'].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(train_data, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('qna_train.json', 'w', encoding='utf-8') as f:\n    json.dump(train, f, ensure_ascii=False, indent=4)\n\nwith open('qna_test.json', 'w', encoding='utf-8') as f:\n    json.dump(test, f, ensure_ascii=False, indent=4)\n","metadata":{"id":"5H3WRYpOiYlM","executionInfo":{"status":"aborted","timestamp":1726356813092,"user_tz":-330,"elapsed":28,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r\"qna_train.json\", \"r\") as read_file:\n    train = json.load(read_file)\n\nwith open(r'qna_test.json', 'r') as read_file:\n    test = json.load(read_file)","metadata":{"id":"n8-R2tDziobI","executionInfo":{"status":"aborted","timestamp":1726356813092,"user_tz":-330,"elapsed":28,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Fine Tuning","metadata":{}},{"cell_type":"code","source":"!pip3 install simpletransformers","metadata":{"id":"buq_rqsqj5KD","executionInfo":{"status":"aborted","timestamp":1726356813092,"user_tz":-330,"elapsed":27,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs","metadata":{"id":"R-vTKQDsjIF5","executionInfo":{"status":"aborted","timestamp":1726356813092,"user_tz":-330,"elapsed":26,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_args = {\n    'overwrite_output_dir': True,\n    'evaluate_during_training' :True,\n    \"max_seq_length\": 150,\n    \"num_train_epochs\": 25, #25, after experimentations\n    \"evaluate_during_training_steps\": 500,\n    \"save_model_every_epoch\": False,\n    \"save_eval_checkpoints\": False,\n    \"n_best_size\":1, #batch_size is another important argument\n    \"train_batch_size\": 20,\n}","metadata":{"id":"19OadxC2jI6_","executionInfo":{"status":"aborted","timestamp":1726356813092,"user_tz":-330,"elapsed":26,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = QuestionAnsweringModel(\"bert\",\n                               \"bert-base-uncased\",\n                               args = train_args,\n                                use_cuda = True)","metadata":{"id":"aKlxzetejLft","executionInfo":{"status":"aborted","timestamp":1726356813093,"user_tz":-330,"elapsed":27,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.train_model(train, eval_data = test, output_dir = '/kaggle/working')","metadata":{"id":"sUn9mzgtpqBz","executionInfo":{"status":"aborted","timestamp":1726356813093,"user_tz":-330,"elapsed":26,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Output Prediction","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/train-data/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Define your mappings of units to standardized terms\nunit_mapping = {\n    r'\\bcm\\b|\\bcms\\b|\\bc.m.\\b': 'centimetre',\n    r'\\bft\\b': 'foot',\n    r'\\bmm\\b|\\bm.m.\\b': 'millimetre',\n    r'\\bm\\b': 'metre',\n    r'\\bin\\b|\\b\"\\b': 'inch',\n    r'\\byd\\b': 'yard',\n    r\"(\\d+)\\s*(cm|cms|c.m.)\\b\": r\"'\\1 centimetre'\",\n    r\"(\\d+)\\s*(ft)\\b\": r\"'\\1 foot'\",\n    r\"(\\d+)\\s*(mm|m.m.)\\b\": r\"'\\1 millimetre'\",\n    r\"(\\d+)\\s*(m)\\b\": r\"'\\1 metre'\",\n    r\"(\\d+)\\s*(in)\\b\": r\"'\\1 inch'\",\n    r\"(\\d+)\\s*(yd)\\b\": r\"'\\1 yard'\",\n    r'(\\bft3\\b|\\bcu ft\\b)': r' cubic foot', \n    r'(\\bμL\\b|\\bμl\\b)': r' microlitre',\n    r'\\bcup\\b': r' cup',\n    r'(\\bfl oz\\b|\\bfl\\. oz\\.\\b)': r' fluid ounce',\n    r'(\\bimperial gallon\\b)': r' imperial gallon',\n    r'\\bgal\\b': r' gallon',\n    r'(\\bpint\\b|\\bpt\\b)': r' pint',\n    r'\\bdl\\b': r' decilitre',\n    r'(\\bl\\b|\\bls\\b)': r' litre',\n    r'(\\bml\\b|\\bmls\\b)': r' millilitre',\n    r'\\bqt\\b': r' quart', \n    r'(\\bcubic in\\b|\\bcu inch\\b|\\bcu in\\b|\\bcui\\b|\\bcu\\. in\\.\\b)': r' cubic inch',\n    r'(\\d+)\\s*(ft3|cu ft)\\b': r'\\1 cubic foot',\n    r'(\\d+)\\s*(μL|μl)\\b': r'\\1 microlitre',\n    r'(\\d+)\\s*(cup)\\b': r'\\1 cup',\n    r'(\\d+)\\s*(fl oz|fl\\. oz\\.)\\b': r'\\1 fluid ounce',\n    r'(\\d+)\\s*(imperial gallon)\\b': r'\\1 imperial gallon',\n    r'(\\d+)\\s*(gal)\\b': r'\\1 gallon',\n    r'(\\d+)\\s*(pint|pt)\\b': r'\\1 pint',\n    r'(\\d+)\\s*(dl)\\b': r'\\1 decilitre',\n    r'(\\d+)\\s*(l|ls)\\b': r'\\1 litre',\n    r'(\\d+)\\s*(ml|mls)\\b': r'\\1 millilitre',\n    r'(\\d+)\\s*(w)\\b': r'\\1 watt',\n    r'(\\d+)\\s*(kw)\\b': r'\\1 kilowatt',\n    r'\\bw\\b|\\bws\\b': 'watt',\n    r'\\bkw\\b|\\bkws\\b': 'kilowatt',\n    r'(\\d+)\\s*(v)\\b': r'\\1 volt',\n    r'(\\d+)\\s*(mv)\\b': r'\\1 millivolt',\n    r'(\\d+)\\s*(kv)\\b': r'\\1 kilovolt',\n    r'\\bv\\b|\\bvs\\b': 'volt',\n    r'\\bmv\\b|\\bmvs\\b': 'millivolt',\n    r'\\bkv\\b|\\bkvs\\b': 'kilovolt',\n    r'(\\d+)\\s*(kg|kgs)\\b': r'\\1 kilogram',\n    r'(\\d+)\\s*(mg|mgs)\\b': r'\\1 milligram',\n    r'(\\d+)\\s*(g|gm)\\b': r'\\1 gram',\n    r'(\\d+)\\s*(oz|ozs)\\b': r'\\1 ounce',\n    r'(\\d+)\\s*(lb|lbs)\\b': r'\\1 pound',\n    r'(\\d+)\\s*(mcg|mcgs)\\b': r'\\1 microgram',\n    r'(\\d+)\\s*(ton)\\b': r'\\1 ton',\n    r'\\bkg\\b|\\bkgs\\b': 'kilogram',\n    r'\\bmg\\b|\\bmgs\\b': 'milligram',\n    r'\\bg\\b|\\bgm\\b': 'gram',\n    r'\\boz\\b|\\bozs\\b': 'ounce',\n    r'\\blb\\b|\\blbs\\b': 'pound',\n    r'\\bmcg\\b|\\bmcgs\\b': 'microgram',\n    r'\\bton\\b': 'ton',\n}\n\ndef replace_units(text, mapping):\n    for pattern, replacement in mapping.items():\n        text = re.sub(pattern, replacement, text)\n    return text\n\n# Function to find numbers and units in separate lists\ndef find_numbers_and_units_after_processing(text, unit_mapping = unit_mapping):\n    # Run the replace_units function first to standardize units\n    processed_text = replace_units(text, unit_mapping)\n    \n    # Split the text into tokens (words and numbers)\n    tokens = re.findall(r'\\d+\\.\\d+|\\d+|\\w+|\\S', processed_text)  # Find numbers, words, and symbols\n    \n    # Lists to hold the results\n    numbers = []\n    units = []\n    number_unit_pairs = []\n\n    # Iterate over tokens and find numbers followed by units\n    i = 0\n    while i < len(tokens):\n        if re.match(r'^\\d+(\\.\\d+)?$', tokens[i]):  # Check if the token is a number (including decimals)\n            # Check if the next token is in the mapping output values (standardized units)\n            if i + 1 < len(tokens) and tokens[i + 1] in unit_mapping.values():\n                # If found, append the number and the unit to their respective lists\n                numbers.append(tokens[i])\n                units.append(tokens[i + 1])\n                # Also, append the combined number-unit pair to the third list\n                number_unit_pairs.append(f\"{tokens[i]} {tokens[i + 1]}\")\n                i += 2  # Skip the next token as it's already processed\n            else:\n                i += 1  # Move to the next token\n        else:\n            i += 1  # Move to the next token if it's not a number\n    return numbers, units, number_unit_pairs\n\ndef yolo_text(text):\n    numbers, units, number_unit_pairs = find_numbers_and_units_after_processing(text)\n    return numbers[0] + ' ' + units[0]\ndef qna_unit(context, value):\n    try:\n        value = int(value)\n    except:\n        return ''\n    numbers, units, number_unit_pairs = find_numbers_and_units_after_processing(text)\n    i=0\n    for num in numbers:\n        if num == value:\n            return number_unit_pairs[i]\n        i+=1\n    return ''\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_yolo(row):\n    img = download_image_to_numpy(row['image_link'])\n    yolo_dict = YOLO_output(img)\n    max_value = -float('inf')  # Initialize to negative infinity\n    max_index = -1\n\n    if len(yolo_dict.get(row['entity_name'].lower())) !=0 :\n        for index, items in enumerate(yolo_dict[row['entity'].lower()]):\n            if items[1] > max_value:\n                max_value = array[1]\n                max_index = index\n    if max_index==-1:\n        return ''\n    \n    else:\n        return yolo_text(yolo_dict.get(row['entity'].lower())[index][0])\n        \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_question(row):\n    if row['entity_name'] == 'item_weight':\n        return \"What is the weight value of the product?\"\n    elif row['entity_name'] == 'maximum_weight_recommendation':\n        return \"What is the maximum weight capacity of the product?\"\n    elif row['entity_name'] == 'wattage':\n        return \"What is the wattage value of the product?\"\n    elif row['entity_name'] == 'voltage':\n        return \"What is the voltage value of the product?\"\n    else:\n        return \"What is the volume value of the product?\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Some changes to be made regarding how answer is returned. Must be returned as a string\ndef predict(context, question):\n    to_predict = [\n    {\n        \"context\": str(context),\n        \"qas\": [\n            {\n                \"question\": str(question),\n                \"id\": \"0\",\n            }\n        ],\n    }]\n \n    answers, probabilities = bert_model.predict(to_predict, n_best_size=1)\n    return qna_text(context, answers[0].get('answer')[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_qna(row):\n    context = get_context(row['image_link']).lower()\n    print(context)\n    question = set_question(row)\n    if context is not None:\n        result = predict(context,question)\n        if result == \"empty\":\n            return \"\"\n        return result\n    return ''\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def assign_prediction(df):\n    # Iterate over each row and apply the condition\n    for index, row in df.iterrows():\n        if row['entity_name'] in ['height', 'width', 'depth']:\n            df.at[index, 'prediction'] = apply_yolo(row)\n        else:\n            df.at[index, 'prediction'] = apply_qna(row)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = assign_prediction(df_test.head(20000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_first_1000.head(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(first_row))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(get_context(first_row['image_link']), set_question(first_row))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(first_row['image_link'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_temp[df_test_temp['entity_name'].isin(['wattage'])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp['question'] = df_test_temp['image_link'].apply(get_context)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def answer_extract(string):\n    return string.split()[0]\n\ndf_test_temp['answer'] = df_test_temp['entity_value'].apply(answer_extract)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp.rename(columns={'question': 'context'}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to check if a string is a number\ndef is_number(string):\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False\n\n# Function to clean the dataframe\ndef clean_dataframe(df):\n    df = df[df['answer'].apply(is_number)]  # Keep only rows where 'answer' is a number\n    return df\n\n# Apply the cleaning function to the DataFrame\ndf_test_temp = clean_dataframe(df_test_temp)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp['answer'] = df_test_temp['answer'].apply(remove_trailing_zeros)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to set the 'question' based on 'entity_name'\ndef set_question(row):\n    if row['entity_name'] == 'item_weight':\n        return \"What is the weight value of the product?\"\n    elif row['entity_name'] == 'maximum_weight_recommendation':\n        return \"What is the maximum weight capacity of the product?\"\n    elif row['entity_name'] == 'wattage':\n        return \"What is the wattage value of the product?\"\n    elif row['entity_name'] == 'voltage':\n        return \"What is the voltage value of the product?\"\n\n# Apply the function to the DataFrame\ndf_test_temp['question'] = df_test_temp.apply(set_question, axis=1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(context, question):\n    to_predict = [\n    {\n        \"context\": context,\n        \"qas\": [\n            {\n                \"question\": question,\n                \"id\": \"0\",\n            }\n        ],\n    }]\n \n    answers, probabilities = bert_model.predict(to_predict, n_best_size=1)\n    return (answers, probabilities)\n\nfirst = df_qna.iloc[9]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(first['context'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(first['question'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(first['answer'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predict(first['context_new'], first['question']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_temp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/train-data/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_row_weight = df_test[df_test['entity_name'] == 'item_weight'].iloc[1]\nfirst_row_max_weight = df_test[df_test['entity_name'] == 'maximum_weight_recommendation'].iloc[1]\nfirst_row_wattage = df_test[df_test['entity_name'] == 'wattage'].iloc[1]\nfirst_row_voltage = df_test[df_test['entity_name'] == 'voltage'].iloc[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = get_context(first_row_voltage['image_link'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(context)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nto_predict = [\n    {\n        \"context\": context,\n        \"qas\": [\n            {\n                \"question\": \"What is the wattage value of the product?\",\n                \"id\": \"0\",\n            }\n        ],\n    }\n]\n \nanswers, probabilities = bert_model.predict(to_predict, n_best_size=1)\nprint(answers, probabilities)","metadata":{"id":"EFTy3jZEwRnm","executionInfo":{"status":"aborted","timestamp":1726356813093,"user_tz":-330,"elapsed":26,"user":{"displayName":"Vijaya Anand","userId":"04509102419784074408"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(first_row_wattage['image_link'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_predict = [\n    {\n        \"context\": get_context(first_row_max_weight['image_link']),\n        \"qas\": [\n            {\n                \"question\": \"What is the maximum weight capacity of the product?\",\n                \"id\": \"0\",\n            }\n        ],\n    }\n]\n \nanswers, probabilities = model.predict(to_predict, n_best_size=1)\nprint(answers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_predict = [\n    {\n        \"context\": get_context(first_row_wattage['image_link']),\n        \"qas\": [\n            {\n                \"question\": \"What is the wattage value of the product?\",\n                \"id\": \"0\",\n            }\n        ],\n    }\n]\n \nanswers, probabilities = model.predict(to_predict, n_best_size=1)\nprint(answers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_predict = [\n    {\n        \"context\": get_context(first_row_voltage['image_link']),\n        \"qas\": [\n            {\n                \"question\": \"What is the voltage value of the product?\",\n                \"id\": \"0\",\n            }\n        ],\n    }\n]\n \nanswers, probabilities = model.predict(to_predict, n_best_size=1)\nprint(answers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}